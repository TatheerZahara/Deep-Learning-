{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XOF3PmROMex"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.preprocessing import LabelBinarizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "id": "--NW-avRhUiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea830e7d-d5ea-45cb-98cd-f5745430e9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 1s 0us/step\n",
            "26435584/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.reshape(X_train, (X_train.shape[0], 784))/255.\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 784))/255."
      ],
      "metadata": {
        "id": "H1rpbRB0hlhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train dimension:');print(X_train.shape)\n",
        "print('Test dimension:');print(X_test.shape)\n",
        "## Changing y's to fit categorical entropy loss\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)\n",
        "print('Train labels dimension:');print(y_train.shape)\n",
        "print('Test labels dimension:');print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiZnc7n_hrIm",
        "outputId": "1191a2cd-f7db-48f3-dc60-637af471714b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dimension:\n",
            "(60000, 784)\n",
            "Test dimension:\n",
            "(10000, 784)\n",
            "Train labels dimension:\n",
            "(60000, 10)\n",
            "Test labels dimension:\n",
            "(10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size_input = 784\n",
        "size_hidden01 = 128\n",
        "size_hidden02=64\n",
        "size_output = 10"
      ],
      "metadata": {
        "id": "HZNmbl_4jHk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(100)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(40)"
      ],
      "metadata": {
        "id": "lHmPfVcxkD6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(tf.keras.Model):\n",
        "  def __init__(self, size_input, size_hidden01, size_hidden02, size_output, device=None):\n",
        "    super(MLP, self).__init__()\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "\n",
        "    # self.size_input = 32\n",
        "    # self.size_hidden = 128\n",
        "    # self.size_output = 1\n",
        "    # self.device = 'gpu'\n",
        "    self.size_input, self.size_hidden01, self.size_hidden02, self.size_output, self.device =\\\n",
        "    size_input, size_hidden01, size_hidden02, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden01]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden01]))\n",
        "    # Initialize weights between hidden layer 1 and hidden 2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden01, self.size_hidden02]))\n",
        "    #Initialize biases for hidden 2\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden02]))\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden02, self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.MLP_variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true, y_pred)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.MLP_variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.MLP_variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"    \n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat = tf.nn.relu(what)\n",
        "    hhat = tf.nn.dropout(hhat, rate=0.5)\n",
        "    #Hidden 2\n",
        "    what1 = tf.matmul(hhat, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what1)\n",
        "    hhat2 = tf.nn.dropout(hhat2, rate=0.5)\n",
        "    # Compute output\n",
        "    out = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    output=tf.nn.softmax(out)\n",
        "    return output"
      ],
      "metadata": {
        "id": "dccGRhJ3hukw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_accuracy(pred_val,true_val):\n",
        "  correct_prediction = tf.equal(tf.argmax(pred_val, 1), tf.argmax(true_val, 1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "  return accuracy\n"
      ],
      "metadata": {
        "id": "UVzC3rtkh-BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 11\n",
        "# Initialize model using GPU\n",
        "#mlp_on_gpu = MLP()\n",
        "mlp_on_gpu = MLP(size_input, size_hidden01, size_hidden02, size_output, device='gpu')\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1000, seed=epoch*(1234)).batch(32)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  \n",
        "  print('Number of Epoch = {} - Categorical Loss:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  preds = mlp_on_gpu.compute_output(X_train)\n",
        "  accuracy_train = find_accuracy(preds,y_train)\n",
        "  accuracy_train = accuracy_train * 100\n",
        "  print (\"Training Accuracy = {}\".format(accuracy_train.numpy()))\n",
        "\n",
        "preds_test = mlp_on_gpu.compute_output(X_test)\n",
        "accuracy_test = find_accuracy(preds_test,y_test)\n",
        "# To keep sizes compatible with model\n",
        "accuracy_test = accuracy_test * 100\n",
        "print (\"Test Accuracy = {}\".format(accuracy_test.numpy()))\n",
        "\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "metadata": {
        "id": "-VwjcAKgz8ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oDNjXZlUcFGp"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MNIST_Dropout.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}